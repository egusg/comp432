{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1EpBDDZXOvUca3hFG7XK6tigJGgQfjw4-","timestamp":1705795476074}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["We here provide description of the notation adopted for this course:"],"metadata":{"id":"5PiE9RSWYi_w"}},{"cell_type":"markdown","source":["# Numbers, Vectors, and Matrices\n","\n","| Symbol   |     Meaning          |\n","|----------|:-------------:|\n","|  $a$ |  Scalar |\n","| $\\mathbf{a}$ |    Vector  <br> $\\mathbf{a}=[a_1, a_2, .., a_N]^T$  |\n","| $\\mathbf{a}^T$ |    Transposed Vector   |\n","| $||\\mathbf{a}||_p$ |    p-norm of the vector $\\mathbf{a}$ <br> $||\\mathbf{a}||_p=\\Big( \\sum_{i=i}^N |a_i|^p\\Big)^{1/p}$   |\n","| $\\mathbf{A}$ | Matrix |\n","| $\\mathbf{A}^T$ | Transposed Matrix |\n","| $\\mathbf{A}^{-1}$ | Inverse Matrix |\n","| $\\mathbf{I}$ | Identity Matrix |\n","| $a_i$ | Element $i$ of the vector $\\mathbf{a}$ |\n","| $A_{ij}$ | Element at raw $i$ and column $j$ of the Matrix $\\mathbf{A}$ |\n","\n","# Calculus\n","\n","| Symbol   |     Meaning          |\n","|-----------------------|:-------------:|\n","|  $\\frac{∂y}{∂ x}$ |  Partial Derivative of $y$ with respect to $x$ |\n","|  $\\nabla_{\\mathbf{x}}y$ |  Gradient of $y$ with respect to $\\mathbf{x}$ |\n","|  $\\mathbf{J}_f(\\mathbf{x})$ |  Jacobian of $f$ with respect to $\\mathbf{x}$ |\n","|  $\\frac{∂^2y}{∂ x^2}$ |  Second Partial Derivative of $y$ with respect to $x$ |\n","|  $H$ |  Hessian Matrix |\n","\n","\n","# Functions\n","\n","| Symbol   |     Meaning          |\n","|-----------------------|:-------------:|\n","|  $ln(x)$ |  Natural Logarithm $x$ |\n","|  $log(x)$ |  Logarithm with base 10 of $x$ |\n","|  $\\sigma(x)$ | Sigmoid (Logistic Function): <br> $\\sigma(x)=\\frac{1}{1 + exp(-x)}$ |\n","\n","\n","# Machine Learning Notation\n","\n","|&nbsp; &nbsp; &nbsp; Symbol &nbsp; &nbsp;&nbsp; |     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Meaning  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        |\n","|-----------------------|:---------------------------:|\n","|  $\\mathbf{X}$ | Input Matrix (Features) <br> $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$ <br> $\\mathbf{X}=[\\mathbf{x}_1, \\mathbf{x}_2, .., \\mathbf{x}_N]^T$ <br> $\\mathbf{X} = \\begin{bmatrix}\n","x_{1,1} & x_{1,2} & ... & x_{1,D}\\\\\n","x_{2,1} & x_{2,2} & ... & x_{2,D}\\\\\n","... & ... & ... & ...\\\\\n","x_{N,1} & x_{N,2} & ... & x_{N,D}\\\\\n","\\end{bmatrix}$|\n","|  D | Dimensionality of the Input vector |\n","|  N | Number of input features |\n","|  $\\mathbf{x}$ | Single input vector (Features)<br>  $\\mathbf{x} \\in \\mathbb{R}^D$ <br> $\\mathbf{x} = [x_1, x_2, .., x_D]^T$ |\n","|  $\\mathbf{y}$ | Label vector (Targets)<br>  $\\mathbf{y} \\in \\mathbb{R}^N$ <br> $\\mathbf{y} = [y_1, y_2, .., x_N]^T$ |\n","|  $\\mathbf{\\hat{y}}$ | Prediction<br>  $\\mathbf{\\hat{y}} \\in \\mathbb{R}^N$ <br> $\\mathbf{\\hat{y}} = [y_1, y_2, .., x_N]^T$ |\n","|  $\\boldsymbol{\\theta}$ | Parameter vector <br>  $\\boldsymbol{\\theta} \\in \\mathbb{R}^M$ <br> $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2, .., x_M]^T$ |\n","|  $\\hat{y}_i$ | Prediction for the input $i$<br>  $\\hat{y}_i = f(\\mathbf{x}_i, \\boldsymbol{\\theta})$ |\n","|  $f(\\mathbf{x}$, $\\boldsymbol{\\theta})$ | Mapping function parametrized by $\\boldsymbol{\\theta}$\n","| $J(\\mathbf{X}, \\mathbf{Y})$, or $J(\\mathbf{X}, \\mathbf{Y},\\boldsymbol{\\theta})$ | Objective Function <br> Often shortened as $J(\\boldsymbol{\\theta})$ or $J$\n","| $L(\\mathbf{X}, \\mathbf{Y})$, or $L(\\mathbf{X}, \\mathbf{Y},\\boldsymbol{\\theta})$ | Loss Functions <br> Often shortened as L($\\boldsymbol{\\theta})$ or L\n","$\\eta$ | Learning rate\n","\n"],"metadata":{"id":"1VGxwTv8YzT5"}}]}