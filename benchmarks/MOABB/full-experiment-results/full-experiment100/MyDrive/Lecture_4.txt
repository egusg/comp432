Lecture 4  Computer Organization and Design                  DKP/Winter 2023
_________

Digital Logic
_____________

We have a dilemma.  An introductory course on computer organization and design
should focus on concepts rather than on engineering detail (I assume students
are not focused on hardware design), and should explain the subject from a
programmer's point of view, and emphasize consequences for programmers.
Normally, I would omit logic design altogether.  But perhaps I can give you a
taste of it without getting bogged down in details.

Logic operators are abstractions for specifying transformations of binary
signals.  The simplest logic circuits are _combinational_.  This means that
they do not operate with _memory_, or _state_.  Combinational circuits may
be abstracted as Boolean functions, which are familiar to us from our study
of _sentential logic_ (also called _propositional calculus_).

The mathematical concept underlying combinational logic is that of _n-place
Boolean function_.  A Boolean function specifies what operation we want a
combinational circuit to compute.  In all but trivial cases, there are many
implementations of a given Boolean function.  Thus, we need terminology to
name the Boolean functions we wish to compute and separate terminology to
describe particular implementations.  I draw (often annotated) diagrams to
describe implementations and use logical formulae from sentential logic to
name Boolean functions. 

Physically, combinational circuits are constructed by wiring together some
number of logic gates, which have names like 'NOT', 'AND', 'OR', 'XOR',
'NAND', 'NOR', and 'XNOR'.  Popular gates correspond to reasonably familiar
binary sentential connectives.  Unpopular gates correspond to other binary
sentential connectives (e.g., '+', "downward arrow", and '|').  In practice,
logic gates may have more than two inputs (and, to a lesser extent, more
than one output).  The semantics of multiple-input 'AND' and 'OR' gates,
and by extension 'NAND' and 'NOR' gates, is reasonably intuitive.  In
contrast, the semantics of multiple-input 'XOR' and 'XNOR' must be carefully
defined.  In this course, I will not draw binary gates with more than two
inputs.

An n-place _Boolean function_ is a map from {T, F}^n to {T, F}.  For each
'n', there are 2^(2^n) n-place Boolean functions.  It is convenient to
identify a connective with the Boolean function it computes.  This gives
us 2^(2^n) n-ary connectives.

Are the sixteen binary connectives _sufficient_ to compute _all_ Boolean
functions?  Might it help to add, say, the ternary majority connective '#',
which is true whenever a majority of its three arguments is true?  No.  We
can compute everything we need from {'~', '/\'} or from {'~', '\/'}, to give
just two examples.  Any such sufficient set is called _complete_.

Here is a correspondance table, which can be used for reference.

logic    name                  gate   meaning                 multiple input
_____    ____                  ____   _______                 ______________
       
T        true                         nullary; always true

inv. T   false                        nullary; always false
 
p        --                    --     'p'

q        --                    --     'q'

~p       not                   NOT    not 'p'

~q       not                   NOT    not 'q'

p /\ q   and                   AND    'p' and 'q'             'all'

p \/ q   or                    OR     'p' or 'q'              'any'

p --> q  conditional           --     if 'p', then 'q'

p <--> q biconditional         XNOR   'p' iff 'q'             'even'

p <-- q  reversed conditional  --     if 'q', then 'p'

p + q    exclusive or          XOR    'p' or 'q', but         'odd'
                                      not both
  |
p v q    nor                   NOR    neither 'p' nor 'q'     'none'

p | q    nand                  NAND   not both 'p' and 'q'    'not all'

p < q    --                    --     (not 'p') and 'q'

p > q    --                    --     'p' and (not 'q')

Remark: None of {'-->', '<--', '<', '>'} are associative or commutative.

We can pair Boolean functions/connectives with their negations:

                                                               |
<T, inv. T>;  <p, ~p>;  <q, ~q>;  <p /\ q, p | q>;  <p \/ q, p v q>;

<p + q, p <--> q>;  <p --> q, p > q>;  <p <-- q, p < q>

To say that the device has no memory is to say that the present output level
depends only on the present input levels (and not on the past history of the
device).

Consider the two-input 'AND' gate:

         +--------+
  p ---> |        |
         |  AND   | ---> p /\ q
  q ---> |        |
         +--------+

We can label the output of a gate or a circuit with a logical formula.

Here is a circuit shown as a labeled diagram:

         +--------+
  p ---> |        |
         |  AND   | ---> p /\ q ---+
  q ---> |        |                |
         +--------+                |     +--------+
                                   +---> |        |
                                         |   OR   | ---> (p /\ q) \/ ~r
                                   +---> |        |
         +--------+                |     +--------+
         |        |                |
  r ---> |  NOT   | ---> ~r -------+ 
         |        |
         +--------+

The same Boolean function can be implemented by many different circuits (i.e.,
different combinations of gates).

Today, all digital design of processors and related hardware systems is done
using a _hardware description language_, along with software tools for
digital analysis and synthesis.  A popular specification language is VHDL.

The next step up from combinational logic is _sequential logic_, which we
need to make registers, caches, and memories.  You may think of a sequential
circuit as a combinational circuit plus some timed state elements.

The behavior of a combinational (memoryless) circuit depends only on its
current inputs, not on past history.  A sequential circuit, on the other
hand, has a finite amount of memory whose content, determined by past inputs,
affects the current output behavior.

Example:                        _____________
          +-----------+        /             \        +-----------+
      d   |           |  d    /               \   d   |           |  d
     ---> | state     | ---> /  combinational  \ ---> | state     | --->
          | element 1 |      \  logic          /      | element 2 |
          |           |       \               /       |           |
          +-----------+        \_____________/        +-----------+
                ^                                           ^
                |                                           |
              clock                                       clock

When a clock signal is received, the value of the data input to a state
element is instantaneously stored in the state element.  This value is
stable until the next clock signal is received.

The state elements provide valid inputs to the combinational logic block.
But the combinational logic itself is not instantaneous; rather, it needs
time to settle.  To ensure that the values written into the state element
on the right are valid, the clock must have a long enough period so that
all the signals in the combinational logic block stabilize, _after_ which
the stable values can be stored into the receiving state element.

Finally, I will give a very brief sketch of how combinational logic may be
used to implement a very simple (and slow) adder.

When two bits are added, the sum is a value in the range [0,2] that can be
represented by a _sum bit_ and a _carry bit_.  A circuit to compute both is
known as a _half adder_.

Exercise: Write a requirements specification for a half adder.

Solution: sum 's' = p + q, carry 'c' = p /\ q

Let me use the symbol '+' to represent 'XOR' (exclusive or).  That is,
'p + q' is equivalent to '(p \/ q) /\ ~(p /\ q)'.  A moment's reflection shows
that a half adder can be implemented with an 'AND' gate and an 'XOR' gate,
since sum = 'p + q' and carry = 'p /\ q'.

Here is a (possibly unnecessary) illustration of a half adder:

              +--------+
  p --o-----> |        |
      |       |  AND   | ---> p /\ q = carry
  q ----o---> |        |
      | |     +--------+
      | |
      | |     +--------+
      +-----> |        |
        |     |  XOR   | ---> p + q = sum
        +---> |        |
              +--------+

Two inputs are fed into each of two gates, giving us two outputs.

By adding a carry input to a half adder, we get a _full adder_.

           c_in
             |
             v
         +--------+
  p ---> |        |
         |   FA   | ---> sum
  q ---> |        |
         +--------+
             |
             v
           c_out

Exercise: Write a requirements specification for a full adder.

Solution: sum 's' = p + q + r, carry 'c_out' = #pqr

Here is a straightforward implementation of a full adder:

         +--------+
  p ---> |        |
         |   HA   | ---> p /\ q ------+
  q ---> |        |                   |
         +--------+                   |
             |                        |           +--------+
       p + q |           +--------+   +---------> |        |
             +---------> |        |               |   OR   | -------------->
                         |   HA   | ------------> |        | (p /\ q) \/
  c_in = r ------------> |        | (p + q) /\ r  +--------+ ((p + q) /\ r)
                         +--------+                          = c_out
                             |
                             v
                      (p + q) + r = sum

Abstractly, a full adder takes three inputs ('p', 'q', and 'r') and computes
two Boolean functions as indicated.  There are many implementations other than
the one shown, and many other ways of naming the two Boolean functions computed.

Some of the other names are:

  sum   = p + q + r |= =| p <--> q <--> r |= =| (+)^3 pqr.       
  c_out = pq \/ r(p + q) |= =| pq \/ pr \/ qr |= =| #pqr.

'(+)^3' is ternary addition modulo 2.  '#' is the ternary majority connective.

Noting the reuse of 'p + q', we get an equivalent diagram:

            ^               c           s = p + q + r
            |               ^
            |               |
            |        s      OR          c = (p /\ q) \/
            |        ^     / \              (r /\ (p + q))
       time |        |    /   \
            |        +   AND   AND
            |       / \ / \   / \       s = (+)^3 pqr
            |      r   +   r p   q
            |         / \              
            |        p   q              c = #pqr

A full adder, connected to a state element for holding the carry bit from
one cycle to the next, functions as a _bit-serial_ adder.  A _ripple-carry
adder_, on the other hand, unfolds the sequential behavior of a bit-serial
adder into space, using a cascade of 'k' full adders to add two k-bit
numbers.  There are a number of faster adders.

Once we have an adder, we can implement, say, a _counter_, and so on.

Faster Circuits
_______________
                                                      _ _
Suppose we want to compute Boolean function r + p q + p q.  I have temporarily
switched notation: now, '+' means '\/', and _overbar_ means not.  There are
two proposed implemenations.  Which one is faster?

         +--------+
  p ---> |        |
         |  AND   | ---> p /\ q ---+
  q ---> |        |                |
         +--------+                |                 +--------+
                                   +---------------> |        |
                                                     |   OR   | ---> p q +
                                                     |        |      _ _
         +--------+                                  +--------+     (p q + r)
  p ---> |  NOT   | ---+                                 ^
         +--------+    |   +--------+   +--------+       |
                       +---|        |   |        |       |
                           |  AND   |---|   OR   |-------+
                       +---|        |   |        |
         +--------+    |   +--------+   +--------+
  q ---> |  NOT   | ---+                    ^
         +--------+                         |
                                            |
  r ----------------------------------------+

Here, the longest path is: ** 'NOT'; 'AND'; 'OR'; 'OR' **.

Equally functionally correct:

         +--------+      +--------+
  p ---> |        |      |        |
         |  AND   | ---> |   OR   | ---+
  q ---> |        |      |        |    |
         +--------+      +--------+    |         +--------+
                             ^         |         |        |
                             |         +-------> |   OR   | ---> (p q + r) +
  r -------------------------+                   |        |      _ _
         +--------+                              +--------+      p q
  p ---> |  NOT   | ---+                             ^
         +--------+    |   +--------+                |
                       +---|        |                |
                           |  AND   |----------------+
                       +---|        |
         +--------+    |   +--------+
  q ---> |  NOT   | ---+
         +--------+

Here, the longest path is: ** 'AND'; 'OR'; 'OR' **, which is clearly better
(even though the 'NOT' gate is quite short in comparison to the others, it is
not infinitely fast!).
                  
Example: Consider p + q + r + s.  There are four inputs.  How shall we lay
out the 'OR' gates so that the circuit settles as rapidly as possible?
Since 'OR' gates take time to settle, we should construct the shallowest
tree possible.  Always, this is the most balanced tree possible.  This will
minimize the longest path from a leaf (variable or subcircuit) to the root
(circuit output).  On the left, the worst tree.  On the right, the best tree.

          +                +         ^
         / \              / \        |
        p   +            /   \       |
           / \          +     +      | time
          q   +        / \   / \     |
             / \      p   q r   s    |
            r   s                    |

Exercise: What is the best layout for

  # pqr |= =| pq + pr + qr

?

De Morgan's Laws
________________

~(p /\ q) |= =| (~p) \/ (~q)

~(p \/ q) |= =| (~p) /\ (~q)

Computer Arithmetic
___________________

We start with addition and subtraction of binary integers.

Humans add binary numbers by acting as bit-serial adders.  With a bit of
practice, this becomes fairly automatic.

We can mathematically describe this process as repeatedly computing two
3-place Boolean functions, where the three arguments are the two bits 'p'
and 'q' plus the carry-in bit 'r = c_in'.  The two Boolean functions are
's' (sum) and 'c' (carry out).  We can specify both functions by means of
a table.

p  q  r  |  s  c
_________|______
0  0  0  |  0  0  s = (+)^3 pqr.
0  0  1  |  1  0  c = #pqr.
0  1  0  |  1  0
0  1  1  |  0  1
1  0  0  |  1  0
1  0  1  |  0  1
1  1  0  |  0  1
1  1  1  |  1  1

Note: Efficient humans do _not_ add by doing table lookup.

Example: 000111 =  7  (carries not shown)
       + 001101 = 13
         ------   --
         010100 = 20

To subtract, we compute a negation and then add.

Example: 000111 =  7  (carries not shown)
       + 111010 = -6
         ------   --
       1|000001 =  1

The interpretation of bit patterns as decimal numbers obviously depends on
the choice of semantics (natural number or two's complement), but the
manipulation of bit patterns does not.  There is one addition table above,
not one table per semantics.

Computers do addition using registers.  We can pretend our two examples take
place in 6-bit registers.  In the addition, there is no carry out from the
result register.  In the subtraction, there _is_ a carry out from the result
register, but the bit pattern in the result register is entirely correct.

By definition, _overflow_ occurs when the result of an operation cannot be
contained in the available hardware, in this case, a 6-bit register.  When
adding operands of opposite signs, overflow cannot occur.

The relationship between carry out and overflow is much simpler in
natural-number semantics.  Here, an n-bit register can store any natural
number strictly less than 2^n.  Therefore, if the true result of an addition
is at least 2^n, then overflow has occurred.  Moreover, in such a case, the
addition algorithm will have produced a carry out.

Example: 111111 = 63  (carries not shown)
       + 111111 = 63
         ------   --
       1|111110 = 62  (correct answer: 126)

Here, we have both carry out and overflow.

Two's-complement semantics is slightly trickier.

What is the largest two's-complement integer that fits into a 6-bit register?
Answer: 2^5 - 1 = 31.  Therefore, we should be able to produce overflow by
adding 16 to itself.

Example: 010000 =  16  (carries not shown)
       + 010000 =  16
         ------
         100000 = -32

Even though there was no carry out from the result register, the correct
answer (viz., 32) cannot fit into a 6-bit register---if we are using
two's-complement semantics to interpret bit patterns.  In natural-number
semantics, of course, a 6-bit register can hold any natural number up to
63.

Returning to two's-complement semantics, we have said that overflow has
occurred.  The absence of a 7th bit means that the sign bit has been set
with the _value_ of the result, rather than with the proper sign of the
result.  We have overwritten the sign bit.

Hence, overflow occurs when adding two positive numbers and the sum is
negative, or vice versa.  This means a carry out has occurred into the sign
bit.

A computer can easily have two separate add instructions, one suitable for
general addition, with the hardware detecting and signaling overflow, and
another suitable for memory-address calculation, with the hardware neither
detecting nor signaling overflow.  Memory addresses are natural numbers,
so the rare overflow problems can be made the programmer's responsibility,
saving a bit of work by the hardware.

Thus, the simple instruction

  subi r1,r1,8

means "add integer -8 (a 16-bit signed _immediate_) to register 'r1', but
ignore overflow".  There aren't two _addition algorithms_, only the two
options of either i) paying attention to overflow or ii) ignoring it, plus
the obvious two options of how to interpret the resulting bit pattern as an
integer.  Since -8 is negative, it would be sign extended to give a 32-bit
signed integer in two's complement.  The addition would take place.  But
the sum, in the case of a memory-address calculation, would be interpreted
with natural-number semantics.  Again, programmer caution is advised.

Arithmetic in 16-bit registers can be shown with four hex digits.

Example: 002c  (+44)
        +ffff  ( -1)
         ----   ---
         002b  (+43)

Multiplication
______________

Multiplication is a bit trickier.  There isn't one way to do it.  The
simplest to explain corresponds to what we learned in lower school (assume
positive numbers):

- put multiplier in a 32-bit shift register
- put multiplicand in the right half of a 64-bit shift register (pad with 32
- initialize a 64-bit accumulator register to zero               zeros)

loop: test lsb of multiplier register
      if lsb = 1, then {
        add multiplicand to accumulator register
      }
      shift multiplicand register 1-bit left (zeros come around)
      shift multiplier register 1-bit right  (fresh bit to inspect)
      if not all 32 multiplier bits have been inspected, then {
        goto loop
      }

Example: 1101  (+13)  multiplicand
       x 1011  (+11)  multiplier
         ----
         1101
        1101
       0000    [This is _not_ the way real computers work!]
      1101
     --------
     10001111  (+143)  final result (product)

The simplest---but inefficient---hardware algorithm puts the multiplicand
in a 64-bit register that is shifted left as each new result is added in.
The sum is accumumulated in a 64-bit product register.  The multiplier
goes in a 32-bit register that is shifted right to serially highlight each
binary digit of the multiplier from right to left.

When the rightmost digit of the (currently shifted) multiplier is 1, we add
the shifted multiplicand to the product (accumulator register).  After this
operation (or non-operation), we shift the multiplicand left and the
multiplier right.

Now, the easiest way to multiply two signed numbers is to convert both
operands to positive numbers, do traditional multiplication, and remember
the signs.

A diagram may help show the intial state:

              shift register 
        +----------------------+            We are multiplying two 32-bit
        |         |            |            numbers, but we are using
        | zero pad| multipli-  |  64 bits   several 64-bit registers.
        |         | cand       |
        +----------------------+
                    |
     +--------+     |
     |        |     |
     |  +----------------------+
     |  |                      |
     |  |         adder        |  64 bits
     |  |                      |
     |  +----------------------+
     |              |                        shift register
     |  +----------------------+            +--------------+
     |  |                      |            |              |
     |  |      accumulator     |   <test>   |  multiplier  |  32 bits
     |  |                      |            |              |
     |  +----------------------+            +--------------+
     |              |
     +--------------+

The shifting is automatic.  The test is primarily to determine if the shifted
multiplicand is to be added to the accumulator (product).

Synthesis Exercises I
_____________________

Definition: A set of connectives is _complete_ if they suffice to express
an arbitrary Boolean function.

'(+)^3' is ternary addition modulo 2.  '(+)^3 pqr' is true iff an odd number
of 'p', 'q', and 'r', is true.

Exercise: Show that {'(+)^3', '/\', 'T', 'F'} is complete.  (No proper subset
is complete).

Exercise: Develop an argument to show that {'(+)^3', '~', 'T', 'F'} is not
complete.

Exercise: Show {'|'} is complete.  Proof:     ~p |= =| p | p.
                                          p \/ q |= =| (~p) | (~q)

Since {'~', '\/'} is complete, and since both connectives can be simulated
using only '|', {'|'} is complete.  QED

Synthesis Exercises II
______________________

Logicians, and circuit designers, are concerned with 'completeness'.  As
defined earlier, a set of connectives is called _complete_ if an arbitrary
Boolean function can be computed using only members of this set.

Well-known examples of complete sets are {'~', '/\'} and {'~', '\/'}.  The
synthesis game started by taking sets of connectives and asking if they were
complete, but may be played without reference to completeness.

1. Show that {'~', '-->'} is complete.
   p --> q |= =| ~p \/ q
  ~p --> q |= =|  p \/ q

2. Show that {'|'} is complete.
   ~p |= =| p | p
   p \/ q |= =| ~p | ~q

3. 'I' is the ternary 'odd' connective.  'Ipqr' is true iff an odd
   number of its arguments is true.  Show that {'I', '/\', 'T', 'F'}
   is complete.
   ~p |= =| IpTF

4. 'P' is the ternary 'even' connective.  'Ppqr' is true iff an even
   number of its arguments is true.  Synthesize 'P' from {'~', '<-->'}.
   Ppqr |= =| ~Ipqr
        |= =| ~[(p + q) + r]
        |= =| (p + q) <--> r
        |= =| ~(p <--> q) <--> r

5. '2!' is the ternary 'precisely two' connective.  '2!pqr' is true
   iff precisely two of its arguments are true.  Show that {'2!', 'T'}
   is complete.
   ~p |= =| 2!pTT
   p /\ q |= =| 2!pq(~T)

6. 'M' is the ternary 'minority' connective.  'Mpqr' is true iff a
   minority of its arguments is true.  Show that {'M', 'F'} is
   complete.
   ~p |= =| MppF
   p /\ q |= =| ~MpqF (#pqF)